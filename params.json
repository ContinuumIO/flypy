{"name":"Numba-nextgen","tagline":"","body":"Next iteration of Numba - The core language ideas\r\n=================================================\r\n\r\nThis document describes the core numba language, which is designed to\r\ngenerate efficient code for general, pythonic code. It will allow us to\r\nimplement most of the features that currently reside in the compiler\r\ndirectly in a runtime. On top of this small core language we can write\r\nmore advanced features such as subtype polymorphism through method\r\ntables.\r\n\r\nI believe we need the following features:\r\n\r\n> -   Methods on user-defined types with specified representations\r\n>     (structs or otherwise)\r\n>\r\n>     > -   Careful control over allocation, mutability and ownership\r\n>\r\n> -   Polymorphism: Generic functions, traits, overloading\r\n>\r\n>     > -   subtyping and inheritance is left to a runtime\r\n>     >     implementation\r\n>     > -   dynamic dispatch for traits is left to a runtime implementation\r\n>     >     :   -   static dispatch only requires some type checking\r\n>     >             support\r\n>     >\r\n> -   User-defined typing rules\r\n> -   Careful control over inlining, unrolling and specialization\r\n> -   Array oriented computing: map/reduce/scan/etc\r\n> -   Extension of the code generator\r\n>\r\nSupport for multi-stage programming would be nice, but is considered a\r\nbonus and deferred to external tools like macropy or mython for now. The\r\ncontrol over optimizations likely provides enough functionality to\r\ngenerate good code.\r\n\r\nThis describes a closed environment with an optionally static, inferred,\r\nlanguage. Static typing will help provide better error messages, and can\r\nprevent inintended use.\r\n\r\nPolymorphism is provided through:\r\n\r\n> -   generic (monomorphized) functions (like C++ templates)\r\n> -   overloading\r\n> -   traits (like interfaces)\r\n> -   subtyping (\"python classes\")\r\n\r\nThis language's goals are ultimate control over performance, and a\r\nlanguage with a well-defined and easily understood subset for the GPU.\r\n\r\nThis language is inspired by the following languages: Rust, Terra,\r\nRPython, Julia, Parakeet, mypy, copperhead. The traits are very similar\r\nto Rust's traits, and are related to type classes in Haskell and\r\ninterfaces in Go.\r\n\r\nHowever, Go interfaces do not allow type-based specialization, and hence\r\nneed runtime type tagging and method dispatch through vtables. Type\r\nconversion between interfaces needs to be runtime-checked type (and new\r\nvtables build at those points, if not cached). Compile-time overloading\r\nis precluded. In Go, interfaces specify what something *can do*, as\r\nopposed to what something *can be*. This can be a useful in a few\r\nsituations, but it means we cannot constrain what things can be (e.g.\r\nany numeric type).\r\n\r\nIn julia we can constrain the types we operate over, which happens\r\nthrough subtyping. E.g.:\r\n\r\n```julia\r\njulia> Int <: Integer\r\ntrue\r\njulia> Int <: Real\r\ntrue\r\njulia> Int <: FloatingPoint\r\nfalse\r\n```\r\n\r\nSo we can define a function which operates only over numbers:\r\n\r\n    julia> function f(x :: Number)\r\n             return x * x\r\n           end\r\n\r\nHere's a the generated code when `x` is an `Int`:\r\n\r\n```llvm\r\njulia> disassemble(f, (Int,))\r\n\r\ndefine %jl_value_t* @f618(%jl_value_t*, %jl_value_t**, i32) {\r\ntop:\r\n  %3 = load %jl_value_t** %1, align 8, !dbg !5256\r\n  %4 = getelementptr inbounds %jl_value_t* %3, i64 0, i32 0, !dbg !5256\r\n  %5 = getelementptr %jl_value_t** %4, i64 1, !dbg !5256\r\n  %6 = bitcast %jl_value_t** %5 to i64*, !dbg !5256\r\n  %7 = load i64* %6, align 8, !dbg !5256\r\n  %8 = mul i64 %7, %7, !dbg !5263\r\n  %9 = call %jl_value_t* @jl_box_int64(i64 %8), !dbg !5263\r\n  ret %jl_value_t* %9, !dbg !5263\r\n}\r\n```\r\n\r\nDisassembling with `Number` generates a much larger chunk of code, which\r\nuses boxed code and ultimately (runtime) multiple dispatch of the `*`\r\nfunction:\r\n\r\n```llvm\r\n%15 = call %jl_value_t* @jl_apply_generic(%jl_value_t* inttoptr (i64 4316726176 to %jl_value_t*), %jl_value_t** %.sub, i32 2), !dbg !5191\r\n```\r\n\r\nHowever, since the implementation of a function is specialized for the\r\nsupertype, it doesn't know the concrete subtype. Type inference can help\r\nprevent these situations and use subtype-specialized code. However, it's\r\nvery easy to make it generate slow code:\r\n\r\n```julia\r\njulia> function g(c)\r\n     if c > 2\r\n       x = 2\r\n     else\r\n       x = 3.0\r\n     end\r\n     return f(x)\r\n   end\r\n\r\njulia> disassemble(g, (Bool,))\r\n```\r\n\r\nThis prints a large chunk of LLVM code (using boxed values), since we\r\nare unifying an Int with a Float. Using both ints, or both floats\r\nhowever leads to very efficient code.\r\n\r\nWhat we want in our language is full control over specialization and\r\nmemory allocation, and easily-understood semantics for what works on the\r\nGPU and what doesn't. The following sections will detail how the above\r\nfeatures will get us there.\r\n\r\n1. User-defined Types\r\n=====================\r\n\r\nWe want to support user-defined types with:\r\n\r\n> -   control over representation\r\n> -   (special) methods\r\n> -   control over mutability\r\n> -   control over stack- vs gc-allocation\r\n\r\nUser-defined types do not support inheritance, which is left to a\r\nruntime implementation. This means that the callees of call-sites are\r\nstatic, and can be called directly. This further means they can be\r\ninlined (something we will exploit).\r\n\r\nThis means that we can even write the most performance-critical parts of\r\nour runtime in this way. The compiler needs to support the following\r\ntypes natively:\r\n\r\n> -   int\r\n> -   float\r\n> -   pointer\r\n> -   struct (with optional methods and properties)\r\n> -   union\r\n> -   array (constant size)\r\n\r\nAnything else is written in the runtime:\r\n\r\n> -   range\r\n> -   complex\r\n> -   array\r\n> -   string/unicode\r\n> -   etc\r\n\r\nThis means we can easily experiment with different data representations\r\nand extend functionality. For instance we can wrap and override the\r\nnative integer multiply to check for overflow, and raise an exception or\r\nissue a warning, or convert to a BigInt.\r\n\r\nRepresentation\r\n--------------\r\n\r\nType representation can be specified through a type 'layout':\r\n\r\n```python\r\n@jit\r\nclass Array(object):\r\n    layout = Struct([('data', 'Char *')])\r\n```\r\n\r\nMutability and Allocation\r\n-------------------------\r\n\r\nEach individual field can be specified to be immutable, or all can be\r\nspecified immutable through a decorator:\r\n\r\n```python\r\n@jit(immutable=True)\r\nclass Array(object):\r\n    ...\r\n```\r\n\r\nIf all fields are immutable, the object can be stack allocated. Unless\r\nmanually specified with `stack=True`, the compiler is free to decide\r\nwhere to allocate the object. This decision may differ depending on the\r\ntarget (cpu or gpu).\r\n\r\nThe `Array` above can be stack-allocated since its fields are immutable\r\n-even though the contained data may not be.\r\n\r\nIf data is mutable, it is allocated on the heap. This means that\r\nallocation of such an object is incompatible with a GPU code generator.\r\nHence, data structures like Arrays must be passed in from the host, and\r\nthings like Lists are not supported. However, one can write a List\r\nimplementation with static size that supports appending a bounded number\r\nof objects.\r\n\r\nWe disallow explicit stack allocation for mutable types for the\r\nfollowing reason:\r\n\r\n```python\r\nx = mutable() # stack allocate\r\ny = x         # copy x into y\r\ny.value = 1   # update y.value, which does not affect x.value\r\n```\r\n\r\nTo make this work one would need to track the lifetimes of the object\r\nitself and all the variables the object is written into, at which point\r\nwe defer you to the Rust programming language. We leave stack allocation\r\nof mutable objects purely as a compile-time optimization.\r\n\r\nDestructors\r\n-----------\r\n\r\nDestructors are supported only for heap-allocated types, irrespective of\r\nmutability. If a \\_\\_del\\_\\_ method is implemented, the object will be\r\nautomatically heap-allocated (unless escape analysis can say otherwise).\r\n\r\nOwnership\r\n---------\r\n\r\nOwnership is tied to mutability:\r\n\r\n> -   Data is owned when (recursively) immutable\r\n> -   Data is shared when it, or some field is mutable (recursively)\r\n\r\nOwned data may be send over a channel to another thread or task. Shared\r\ndata cannot be send, unless explicitly marked as a safe operation:\r\n\r\n    channel.send(borrow(x))\r\n\r\nThe user must guarantee that 'x' stays alive while it is consumed. This\r\nis useful for things like parallel computation on arrays.\r\n\r\nType Parameters\r\n---------------\r\n\r\nUser-defined types are parameterizable:\r\n\r\n```python\r\n@jit('Array[Type dtype, Int ndim]')\r\nclass Array(object):\r\n    ...\r\n```\r\n\r\nParameters can be types or values of builtin type int. This allows\r\nspecialization for values, such as the dimensionality of an array:\r\n\r\n```python\r\n@jit('Array[Type dtype, Int ndim]')\r\nclass Array(object):\r\n\r\n    layout = Struct([('data', 'Char *'), ('strides', 'Tuple[Int, ndim]')])\r\n\r\n    @signature('Tuple[Int, ndim] -> T')\r\n    def __getitem__(self, indices):\r\n        ...\r\n```\r\n\r\nThis specifies that we take a `Tuple` of `Int`s an size `ndim` as\r\nargument, and return an item of type `T`. The `T` and `ndim` are\r\nresolved as type parameters, which means they specify concrete types in\r\nthe method signature.\r\n\r\nThe type can now be used as follows:\r\n\r\n```python\r\nmyarray = Array[Double, 2]()\r\n```\r\n\r\nThis will mostly appear in (numba) library code, and not in user-written\r\ncode, which uses higher-level APIs that ultimately construct these\r\ntypes. E.g.:\r\n\r\n```python\r\n@overload(np.ndarray)\r\ndef typeof(array):\r\n    return Array[typeof(array.dtype), array.ndim]\r\n\r\n@overload(np.dtype)\r\ndef typeof(array):\r\n    return { np.double: Double, ...}[array.dtype]\r\n```\r\n\r\n2. Polymorphism\r\n===============\r\n\r\nSupported forms of polymorphism are generic functions, overloading,\r\ntraits and subtyping and inheritance.\r\n\r\nGeneric Functions (@autojit)\r\n----------------------------\r\n\r\nGeneric functions are like `@autojit`, they provide specialized code for\r\neach unique combination of input types. They may be optionally typed and\r\nconstrained (through traits).\r\n\r\n```python\r\n@jit('(a -> b) -> [a] -> [b]')\r\ndef map(f, xs):\r\n    ...\r\n```\r\n\r\nThis specifies a map implementation that is specialized for each\r\ncombination of type instances for type variables \\`a\\` and \\`b\\`. Type\r\nvariables may be further constrained through traits, in a similar way to\r\nRust's traits\r\n([http://static.rust-lang.org/doc/tutorial.html\\#traits)](http://static.rust-lang.org/doc/tutorial.html#traits)),\r\nallowing you to operate for instance only on arrays of numbers, or\r\narrays of floating point values.\r\n\r\nTraits\r\n------\r\n\r\nTraits specify an interface that value instances implement. Similarly to\r\nRust's traits and Haskell's type classes, they are a form of bounded\r\npolymorphism, allowing users to constrain type variables (\"this function\r\noperates on floating point values only\").\r\n\r\nThey also specify a generic interface that objects can implement.\r\nClasses can declare they belong to a certain trait, allowing any\r\ninstance of the class to be used through the trait:\r\n\r\n```python\r\n@jit('(a -> b) -> Iterable[a] -> [b]')\r\ndef map(f, xs):\r\n    ...\r\n```\r\n\r\nOur map now takes an iterable and returns a list. Written this way, a\r\nsingle map implementation now works for *any* iterable. Any value\r\nimplementing the Iterable trait can now be used:\r\n\r\n```python\r\n@jit('Array[Type dtype, Int ndim]')\r\nclass Array(Iterable['dtype']):\r\n    ...\r\n```\r\n\r\nWe can now use map() over our array. The generated code must now insert\r\na \\`conversion\\` between `Array[dtype, ndim]` and trait\r\n`Iterable[dtype]`, which concretely means packing up a vtable pointer\r\nand a boxed Array pointer. This form of polymorphism will likely be\r\n*incompatible with the GPU backend*. However, we can still use our\r\ngeneric functions by telling the compiler to specialize on input types:\r\n\r\n```python\r\n@specialize.argtypes('f', 'xs')\r\n@jit('(a -> b) -> Iterable[a] -> [b]')\r\ndef map(f, xs):\r\n    ...\r\n```\r\n\r\nAlternatively, we can allow them to simply constrain type variables, and\r\nnot actually specify the type as the trait. The type is supplied instead\r\nby the calling context:\r\n\r\n```python\r\n@signature('(it:Iterable[a]) => (a -> b) -> it -> [b]')\r\ndef map(f, xs):\r\n    ...\r\n```\r\n\r\nThe constraints are specified in similar way to Haskell's type classes.\r\nThe only implementation required in the compiler to support this is the\r\ntype checking feature, otherwise it's entirely the same as generic\r\nfunctions above. Multiple constraints can be expressed, e.g.\r\n`(it:Iterable[a], a:Integral)`. Alternative syntax could be '(a -\\> b)\r\n-\\> lst : Iterable[a] -\\> [b]', but this is less clear when 'it' is\r\nreused elsewhere as a type variable.\r\n\r\nTraits can further use inheritance and have default implementations.\r\nThis can be trivially implemented at the Python level, requiring no\r\nspecial knowledge in the compiler.\r\n\r\nOverloading and Multiple-dispatch\r\n---------------------------------\r\n\r\nThese mechanisms provide compile-time selection for our language. It is\r\nrequired to support the compiled `convert` from section 3, and necessary\r\nfor many implementations, e.g.:\r\n\r\n```python\r\n@jit('Int -> Int')\r\ndef int(x):\r\n    return x\r\n\r\n@jit('String -> Int')\r\ndef int(x):\r\n    return parse_int(x)\r\n```\r\n\r\nOverloading is also provided for methods:\r\n\r\n```python\r\n@jit\r\nclass SomeNeatClass(object):\r\n    @signature('Int -> Int')\r\n    def __add__(self, other):\r\n        return self.value + other\r\n\r\n    @signature('String -> Int')\r\n    def __add__(self, other):\r\n        return str(self.value) + other\r\n```\r\n\r\nWe further need a way to \"overload\" python functions to provide a way to\r\nprovide alternative implementations or to type it. We can easily provide\r\nimplementations for all builtins:\r\n\r\n```python\r\npytypedef(builtins.int, int)\r\n```\r\n\r\n3. User-defined Typing Rules\r\n============================\r\n\r\nI think Julia does really well here. Analogously we define three\r\nfunctions:\r\n\r\n> -   typeof(pyobj) -\\> Type\r\n> -   convert(Type, Value) -\\> Value\r\n> -   unify(Type, Type) -\\> Type\r\n\r\nThe `convert` function may make sense as a method on the objects\r\ninstead, which is more pythonic, e.g. `__convert__`. `unify` does not\r\nreally make sense as a method since it belongs to neither of the two\r\narguments.\r\n\r\nUnify takes two types and returns the result type of the given types.\r\nThis result type can be specified by the user. For instance, we may\r\ndetermine that `unify(Int, Float)` is `Union(Int, Float)`, or that it is\r\n`Float`. The union will give the same result as Python would, but it is\r\nalso more expensive in the terms of the operations used on it (and\r\npotentially storage capacity). Unify is used on types only at control\r\nflow merge points.\r\n\r\nA final missing piece are a form of ad-hoc polymophism, namely\r\ncoercions. This is tricky in the presence of overloading, where multiple\r\ncoercions are possible, but only a single coercion is preferable. E.g.:\r\n\r\n```python\r\n@overload('Float32 -> Float32 -> Float32')\r\ndef add(a, b):\r\n    return a + b\r\n\r\n@overload('Complex64 -> Complex64 -> Complex64')\r\ndef add(a, b):\r\n    return a + b\r\n```\r\n\r\nWhich implementation is `add(1, 2)` supposed to pick, `Int` freely\r\ncoerces to both `Float32` and `Complex64`? Since we don't want built-in\r\ncoercion rules, which are not user-overridable or extensible, we need\r\nsome sort of coercion function. We choose a function\r\n`coercion_distance(src_type, dst_type)` which returns the supposed\r\ndistance between two types, or raises a TypeError. Since this is not\r\ncompiled, we decide to not make it a method of the source type.\r\n\r\n```python\r\n@overload(Int, Float)\r\ndef coercion_distance(int_type, float_type):\r\n    return ...\r\n```\r\n\r\nThese functions are used at compile time to determine which conversions\r\nto insert, or whether to issue typing errors.\r\n\r\n4. Optimization and Specialization\r\n==================================\r\n\r\nWe need to allow careful control over optimizations and code\r\nspecialization. This allows us to use the abstractions we need, without\r\npaying them if we know we can't afford it. We propose the following\r\nintrinsics exposed to users:\r\n\r\n> -   `for x in unroll(iterable): ...`\r\n> -   `@specialize.arg(0)`\r\n\r\nUnrolling\r\n---------\r\n\r\nThe first compiler intrinsic allows unrolling over constant iterables.\r\nFor instance, the following would be a valid usage:\r\n\r\n```python\r\nx = (1, 2, 3)\r\nfor i in unroll(x):\r\n    ...\r\n```\r\n\r\nAn initial implementation will likely simply recognize special container\r\ntypes (Tuple, List, etc). Later we may allow arbitrary (user-written!)\r\niterables, where the result of `len()` must be ultimately constant\r\n(after inlining and register promotion).\r\n\r\nSpecialization\r\n--------------\r\n\r\nThe ability to specialize on various things, similar to specialization\r\nin rpython (`rpython/rlib/objectmodel.py`).\r\n\r\nThese decorators should also be supported as extra arguments to\r\n`@signature` etc.\r\n\r\n5. Data-parallel Operators\r\n==========================\r\n\r\nParakeet and copperhead do this really well. We need map, reduce, zip,\r\nlist comprehensions, etc.\r\n\r\n6. Extension of the Code Generator\r\n==================================\r\n\r\nWe can support an `@opaque` decorator that marks a function or method as\r\n\"opaque\", which means it must be resolved by the code generator. A\r\ndecorator `@codegen(thefunc)` registers a code generator function for\r\nthe function or method being called:\r\n\r\n```python\r\n@jit('Int[Int size]')\r\nclass Int(object):\r\n    @opague('Int -> Int', eval_if_const=True)\r\n    def __add__(self, other):\r\n        return a + b\r\n\r\n@codegen(Int.__add__)\r\ndef emit_add(codegen, self, other):\r\n    # 'self' and 'other' are (typed) pykit values\r\n    return codegen.builder.add(self, other)\r\n```\r\n\r\nThis can also be useful to retain high-level information, instead of\r\nexpanding it out beforehand. This can enable high-level optimizations,\r\ne.g. consider the following code:\r\n\r\n```python\r\nL = []\r\nfor i in range(n):\r\n    L.append(i)\r\n\r\nL = map(f, L)\r\n```\r\n\r\nIf we expand `L = []` and `L.append(i)` into memory allocations and\r\nresizes before considering the `map`, we forgo a potential optimization\r\nwhere the compiler performs loop fusion and eliminates the intermediate\r\nlist.\r\n\r\nSo an opague function *may* have an implementation, but it may be\r\nresolved at a later stage during the pipeline if it is still needed:\r\n\r\n```python\r\n@codegen(List.__init__)\r\ndef emit_new_list(codegen, self):\r\n    return codegen.builder.new_list(self.type)\r\n\r\n@llcodegen('new_list')\r\ndef emit_new_list(codegen, self):\r\n    return codegen.gen_call(List.__init__)\r\n```\r\n\r\nThis should be done with low-level code that doesn't need further\r\nhigh-level optimizations. Users must also ensure this process terminates\r\n(there must be no cycles the call graph).\r\n\r\nConclusion\r\n==========\r\n\r\nThe mechanisms above allow us to easily evaluate how code will be\r\ncompiled, and asses the performance implications. Furthermore, we can\r\neasily see what is GPU incompatible, i.e. anything that:\r\n\r\n> -   uses CFFI (this implies use of Object, which is implemented in\r\n>     terms of CFFI)\r\n> -   uses traits that don't merely constrain type variables\r\n> -   allocates anything mutable\r\n\r\nEverything else should still work.\r\nNumba Runtime\r\n=============\r\n\r\nNearly all built-in data types are implemented in the runtime.\r\n\r\nGarbage Collector\r\n=================\r\n\r\nTo support mutable heap-allocated types, we need a garbage collector. To\r\nget started quickly we can use Boehm or reference counting. We will want\r\nto port one of the available copying collectors and use a shadowstack or\r\na lazy pointer stack (for bonus points). The GC should then be local to\r\neach thread, since there is no shared state between threads (only owned\r\nand borrowed data is allowed).\r\n\r\nGarbage collection is abstracted by pykit.\r\n\r\nExceptions\r\n==========\r\n\r\nExceptions are also handled by pykit. We can implement several models,\r\ndepending on the target architecture:\r\n\r\n> -   costful (error return codes)\r\n>     :   -   This will be used on the GPU\r\n>\r\n> -   zero-cost\r\n>     :   -   This should be used where supported. We will start with\r\n>             costful\r\n>\r\n> -   setjmp/longjmp\r\n>     :   -   This will need to happen for every stack frame in case of\r\n>             a shadow stack\r\n>\r\nLocal exception handling will be translated to jumps. This is not\r\ncontrived, since we intend to make heavy use of inlining:\r\n\r\n```python\r\nwhile 1:\r\n    try:\r\n        i = x.__next__()\r\n    except StopIteration:\r\n        break\r\n```\r\n\r\n`x.__next__()` may be inlined (and will be in many instances, like\r\nrange()), and the `raise StopIteration` will be translated to a jump.\r\nControl flow simplification can further optimize the extra jump (jump to\r\nbreak, break to loop exit).\r\n\r\nThreads\r\n=======\r\n\r\nAs mentioned in the core language overview, memory is not shared unless\r\nborrowed. This process is unsafe and correctness must be ensured by the\r\nuser. Immutable data can be copied over channels between threads. Due to\r\na thread-local GC, all threads can run at the same time and allocate\r\nmemory at the same time.\r\n\r\nWe will remove prange and simply use a parallel map with a closure.\r\n\r\nTraits\r\n======\r\n\r\nTraits are mostly a compile-time type-checking detail and some simple\r\nruntime decorator support. Traits with dynamic dispatch require vtables,\r\nsomething we can implement in the runtime as well:\r\n\r\n> [https://github.com/zdevito/terra/blob/master/tests/lib/golike.t](https://github.com/zdevito/terra/blob/master/tests/lib/golike.t)\r\n\r\nExtension Types\r\n===============\r\n\r\nExtension types are currently built on top of CPython objects. This\r\nshould be avoided. We need to decouple numba with anything CPython, for\r\nthe sake of portability as well as pycc.\r\n\r\nExtension types can also easily be written in the runtime:\r\n\r\n> -   `unify()` needs to return the supertype or raise a type error\r\n> -   `convert(obj, Object)` needs to do a runtime typecheck\r\n> -   `coerce_distance` needs to return a distance for how far the\r\n>     supertype is up the inheritance tree\r\n\r\nThe approach is simple: generate a wrapper method for each method in the\r\nextension type that does a vtable lookup.\r\n\r\nClosures\r\n========\r\n\r\nThis time we will start with the most common case: closures consumed as\r\ninner functions. This means we don't need dynamic binding for our cell\r\nvariables, and we can do simple lambda lifting instead of complicated\r\nclosure conversion. This also trivially works on the GPU, allowing one\r\nto use map, filter etc, with lambdas trivially.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}